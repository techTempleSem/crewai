{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "TAVILY_API_KEY = os.getenv(\"TAVILY_API_KEY\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "os.environ[\"TAVILY_API_KEY\"]=TAVILY_API_KEY\n",
    "os.environ['OPENAI_API_KEY']=OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]\n",
    "\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size = 250, chunk_overlap = 0\n",
    ")\n",
    "\n",
    "docs_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=docs_splits,\n",
    "    collection_name=\"rag-chroma\",\n",
    "    embedding=OpenAIEmbeddings()\n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\semte\\anaconda3\\envs\\crewai\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3577: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Binary score for relevant to the question, 'yes' or 'no'\"\"\"\n",
    "    binary_score: str = Field(\n",
    "        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "structed_llm_grader = llm.with_structured_output(GradeDocuments)\n",
    "\n",
    "system=\"\"\"\n",
    "You are a grader assessing relevance of a retrieved document to a user quetion.\n",
    "If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant.\n",
    "Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\n",
    "\"\"\"\n",
    "\n",
    "grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Retrueved document: \\n\\n {document} \\n\\n User question: {question}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "retrieval_grader = grade_prompt | structed_llm_grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "system=\"\"\"\n",
    "You are a question re-writer that converts an input question to a better version that is optimized for web search.\n",
    "Look at the input and try to reason about the underlying semantic intent / meaning.\n",
    "\"\"\"\n",
    "\n",
    "re_write_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Here is the initial question: \\n\\n {question} \\n Formulate an improved question.\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_rewriter = re_write_prompt | llm  | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "web_search_tool = TavilySearchResults(k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        generation: LLM generation\n",
    "        web_search: whether to add search\n",
    "        documents: list of documents\n",
    "    \"\"\"\n",
    "    question: str\n",
    "    generation: str\n",
    "    web_search: str\n",
    "    documents: List[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "def retrieve(state):\n",
    "    \"\"\"\n",
    "    Retrueve documents\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"--- RETRIEVE ---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    documents = retriever.get_relevant_documents(question)\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    print(\"--- GENERATE ---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
    "    return {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
    "\n",
    "def grade_documents(state):\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates documents key with only filtered relevant documents\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"--- CHECK DOCUMENT RELEVANCE TO QUESTION ---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    filter_docs = []\n",
    "    web_search = \"no\"\n",
    "\n",
    "    for d in documents:\n",
    "        score = retrieval_grader.invoke({\"question\": question, \"document\": d.page_content})\n",
    "        grade = score.binary_score\n",
    "        if grade == \"yes\":\n",
    "            print(\"--- GRADE: RELEVANT ---\")\n",
    "            filter_docs.append(d)\n",
    "        else:\n",
    "            print(\"--- GRADE: NOT RELEVANT ---\")\n",
    "            web_search = 'yes'\n",
    "            continue\n",
    "        \n",
    "    return {\"documents\": filter_docs, \"question\": question, \"web_search\": web_search}\n",
    "\n",
    "def transform_query(state):\n",
    "    \"\"\"\n",
    "    Transform the query to produce a better question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates question key with a re-phrased question\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"--- TRANSFORM QUERY ---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    better_question = question_rewriter.invoke({\"question\": question})\n",
    "    return {\"documents\": documents, \"question\": better_question}\n",
    "\n",
    "def web_search(state):\n",
    "    \"\"\"\n",
    "    Web search based on the re-phrased question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates documents key with appended web results\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"--- WEB SEARCH ---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    docs = web_search_tool.invoke({\"query\": question})\n",
    "    web_results = \"\\n\".join(d[\"content\"] for d in docs)\n",
    "    web_results = Document(page_content=web_results)\n",
    "    documents.append(web_results)\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "def decide_to_generate(state):\n",
    "    \"\"\"\n",
    "    Determines whether to generate an answer, or re-generate a question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Binary decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"--- ASSESS GRADED DOCUMENTS ---\")\n",
    "    web_search = state[\"web_search\"]\n",
    "\n",
    "    if web_search == \"yes\":\n",
    "        print(\"--- TRANSFORM QUESTION ---\")\n",
    "        return \"transform_query\"\n",
    "    else:\n",
    "        print(\"--- GENERATE ---\")\n",
    "        return \"generate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, START, StateGraph\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "workflow.add_node(\"retrieve\", retrieve)\n",
    "workflow.add_node(\"grade_documents\", grade_documents)\n",
    "workflow.add_node(\"generate\", generate)\n",
    "workflow.add_node(\"transform_query\", transform_query)\n",
    "workflow.add_node(\"web_search_node\", web_search)\n",
    "\n",
    "workflow.add_edge(START, \"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"transform_query\": \"transform_query\",\n",
    "        \"generate\": \"generate\"\n",
    "    }\n",
    ")\n",
    "workflow.add_edge(\"transform_query\", \"web_search_node\")\n",
    "workflow.add_edge(\"web_search_node\", \"generate\")\n",
    "workflow.add_edge(\"generate\", END)\n",
    "\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- RETRIEVE ---\n",
      "Node 'retrieve':\n",
      "\n",
      "----\n",
      "\n",
      "--- CHECK DOCUMENT RELEVANCE TO QUESTION ---\n",
      "--- GRADE: NOT RELEVANT ---\n",
      "--- GRADE: NOT RELEVANT ---\n",
      "--- GRADE: RELEVANT ---\n",
      "--- GRADE: NOT RELEVANT ---\n",
      "--- ASSESS GRADED DOCUMENTS ---\n",
      "--- TRANSFORM QUESTION ---\n",
      "Node 'grade_documents':\n",
      "\n",
      "----\n",
      "\n",
      "--- TRANSFORM QUERY ---\n",
      "Node 'transform_query':\n",
      "\n",
      "----\n",
      "\n",
      "--- WEB SEARCH ---\n",
      "Node 'web_search_node':\n",
      "\n",
      "----\n",
      "\n",
      "--- GENERATE ---\n",
      "Node 'generate':\n",
      "\n",
      "----\n",
      "\n",
      "The different types of agent memory in artificial intelligence include short-term memory, which is used for in-context learning, and long-term memory, which allows agents to retain and recall information over extended periods. Additionally, agents may utilize semantic memory to store general knowledge about the world. These memory types enhance the agent's ability to interact dynamically with their environment.\n"
     ]
    }
   ],
   "source": [
    "inputs = {\"question\": \"What are the types of agent memory?\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        print(f\"Node '{key}':\")\n",
    "    print(\"\\n----\\n\")\n",
    "print(value[\"generation\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crewai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
